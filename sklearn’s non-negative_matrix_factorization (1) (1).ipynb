{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e422575e-1b6e-42b4-a6ee-bff4deef8f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q kagglehub\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34896955-896d-4291-b96f-510a21324352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved credentials to: /Users/cynthiamcginnis/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, stat, pathlib\n",
    "\n",
    "src = \"/Users/cynthiamcginnis/Downloads/kaggle (2).json\"   # your file\n",
    "dst_dir = pathlib.Path.home()/\".kaggle\"\n",
    "dst = dst_dir/\"kaggle.json\"\n",
    "\n",
    "dst_dir.mkdir(exist_ok=True)\n",
    "shutil.copy2(src, dst)\n",
    "\n",
    "# chmod 600\n",
    "dst.chmod(stat.S_IRUSR | stat.S_IWUSR)\n",
    "\n",
    "print(\"Saved credentials to:\", dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef352e1-d574-4c0b-8b4c-35fc8157e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration values from /Users/cynthiamcginnis/.kaggle\n",
      "- username: cyleemcginnis\n",
      "- path: None\n",
      "- proxy: None\n",
      "- competition: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "# install kaggle if needed\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"])\n",
    "\n",
    "# simple sanity check: this should print your Kaggle account info JSON\n",
    "subprocess.check_call([\"kaggle\", \"config\", \"view\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba47805-52ad-4718-a9b5-c702b3788776",
   "metadata": {},
   "source": [
    "# Part II — MovieLens 1M with sklearn NMF (RMSE)\n",
    "\n",
    "**Goal.** Load the MovieLens-1M ratings, predict missing ratings using **matrix factorization (NMF)** from `sklearn`, and report **RMSE** on a proper **train/test split**.  \n",
    "We’ll also compute simple **baselines** for comparison (global mean, user mean, item mean).\n",
    "\n",
    "**Key constraint.** `sklearn.decomposition.NMF` cannot natively handle missing values or observation masks; it minimizes error over *all* entries (including the many zeros we use to indicate \"missing\"). This is a core **limitation** we’ll discuss after reporting metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0828e4-d7c8-416d-ac44-1c29da8118cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/odedgolden/movielens-1m-dataset\n",
      "License(s): unknown\n",
      "Downloading movielens-1m-dataset.zip to data_movielens1m\n",
      "\n",
      "Files in data folder:\n",
      " README\n",
      "movies.dat\n",
      "ratings.dat\n",
      "users.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.83M/5.83M [00:00<00:00, 2.28GB/s]\n"
     ]
    }
   ],
   "source": [
    "# Step A — Download and unpack the dataset via Kaggle CLI \n",
    "import os, zipfile, glob, subprocess, sys, pandas as pd, numpy as np\n",
    "\n",
    "DATA_DIR = \"data_movielens1m\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# install kaggle if needed (no-op if already present)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"])\n",
    "\n",
    "# download (idempotent: if the zip already exists, Kaggle keeps it)\n",
    "subprocess.check_call([\n",
    "    \"kaggle\", \"datasets\", \"download\",\n",
    "    \"-d\", \"odedgolden/movielens-1m-dataset\",\n",
    "    \"-p\", DATA_DIR\n",
    "])\n",
    "\n",
    "# unzip all zips in DATA_DIR\n",
    "for zf in glob.glob(os.path.join(DATA_DIR, \"*.zip\")):\n",
    "    with zipfile.ZipFile(zf, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    os.remove(zf)\n",
    "\n",
    "print(\"Files in data folder:\\n\", \"\\n\".join(sorted(os.listdir(DATA_DIR))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f39cb2-7a2e-465e-a9d6-c5b51acaffcd",
   "metadata": {},
   "source": [
    "## Step B — Load ratings \n",
    "\n",
    "Standardize the columns to: **userId, movieId, rating, timestamp**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6c4b77c-cc48-4962-94aa-750f0e0f9468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "(1000209, 4) 6040 3706\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os, glob\n",
    "\n",
    "def load_movielens_ratings(data_dir):\n",
    "    # try .csv first\n",
    "    csvs = glob.glob(os.path.join(data_dir, \"*ratings*.csv\"))\n",
    "    if csvs:\n",
    "        df = pd.read_csv(csvs[0])\n",
    "        # normalize column names\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        # common names: userId, movieId, rating, timestamp\n",
    "        return df.rename(columns={cols.get('userid','userId'): 'userId',\n",
    "                                  cols.get('movieid','movieId'): 'movieId',\n",
    "                                  cols.get('rating','rating'): 'rating',\n",
    "                                  cols.get('timestamp','timestamp'): 'timestamp'})[['userId','movieId','rating','timestamp']]\n",
    "    # try .dat (MovieLens 1M uses '::')\n",
    "    dats = glob.glob(os.path.join(data_dir, \"*ratings*.dat\"))\n",
    "    if dats:\n",
    "        df = pd.read_csv(dats[0], sep=\"::\", engine=\"python\",\n",
    "                         names=[\"userId\",\"movieId\",\"rating\",\"timestamp\"])\n",
    "        return df\n",
    "    raise FileNotFoundError(\"Could not find ratings file (.csv or .dat).\")\n",
    "\n",
    "ratings = load_movielens_ratings(DATA_DIR)\n",
    "print(ratings.head())\n",
    "print(ratings.shape, ratings['userId'].nunique(), ratings['movieId'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73a4a5-e2a2-4a67-9385-d2a5d0f35fc4",
   "metadata": {},
   "source": [
    "## Step C — Train/Test split **by user** (prevent leakage)\n",
    "\n",
    "We do a per-user split: for each user, hold out 20% of their ratings for **test** and keep 80% for **train**.  \n",
    "This mirrors “predict unseen ratings for users we already know,” which is standard for CF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d54ccdb3-5cf0-4f56-904d-2694d01bca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test sizes: (797758, 4) (202451, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sort for reproducibility\n",
    "ratings = ratings.sort_values(['userId','timestamp']).reset_index(drop=True)\n",
    "\n",
    "# build per-user train/test split indices\n",
    "train_idx = []\n",
    "test_idx  = []\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "for uid, grp in ratings.groupby('userId', sort=False):\n",
    "    idx = grp.index.to_numpy()\n",
    "    if len(idx) <= 5:\n",
    "        # tiny history users: keep all in train\n",
    "        train_idx.extend(idx.tolist())\n",
    "        continue\n",
    "    # stratified by nothing here; just a random sample per user\n",
    "    tr, te = train_test_split(idx, test_size=0.2, random_state=42)\n",
    "    train_idx.extend(tr.tolist())\n",
    "    test_idx.extend(te.tolist())\n",
    "\n",
    "train_mask = np.zeros(len(ratings), dtype=bool); train_mask[train_idx] = True\n",
    "test_mask  = np.zeros(len(ratings), dtype=bool); test_mask[test_idx]  = True\n",
    "\n",
    "train = ratings[train_mask].copy()\n",
    "test  = ratings[test_mask].copy()\n",
    "\n",
    "print(\"Train/test sizes:\", train.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6368c29-773f-42c4-9dc8-5e37541aded2",
   "metadata": {},
   "source": [
    "## Step D — Build user–item matrices and fit **sklearn NMF**\n",
    "\n",
    "- Construct a sparse **R_train** matrix (users × items) with observed train ratings; zeros elsewhere.\n",
    "- Fit `NMF(n_components = k)` on **R_train** (note: NMF “sees” zeros as real zeros, which is a limitation).\n",
    "- Reconstruct `R_hat = W @ H` and evaluate **RMSE** on the **test** user-item pairs only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee422a7-0788-4249-9880-450233c1f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn NMF (k=20) — Test RMSE: 2.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cynthiamcginnis/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# map ids to consecutive indices\n",
    "user_index = {u:i for i,u in enumerate(ratings['userId'].unique())}\n",
    "item_index = {m:i for i,m in enumerate(ratings['movieId'].unique())}\n",
    "\n",
    "n_users = len(user_index)\n",
    "n_items = len(item_index)\n",
    "\n",
    "def to_csr(df):\n",
    "    ui = df['userId'].map(user_index).to_numpy()\n",
    "    ii = df['movieId'].map(item_index).to_numpy()\n",
    "    vv = df['rating'].astype(float).to_numpy()\n",
    "    return csr_matrix((vv, (ui, ii)), shape=(n_users, n_items))\n",
    "\n",
    "R_train = to_csr(train)\n",
    "\n",
    "# --- Hyperparameter: number of latent factors (topics) ---\n",
    "k = 20  # start with 20; you can try [10, 20, 40, 80]\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=k,\n",
    "    init=\"nndsvda\",\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    alpha_W=0.0, alpha_H=0.0, l1_ratio=0.0\n",
    ")\n",
    "\n",
    "W = nmf.fit_transform(R_train)   # (n_users × k)\n",
    "H = nmf.components_              # (k × n_items)\n",
    "R_hat = W @ H                    # dense reconstruction\n",
    "\n",
    "# clip to rating scale\n",
    "R_hat = np.clip(R_hat, 1.0, 5.0)\n",
    "\n",
    "# Build arrays of predictions on the **test** pairs\n",
    "u_te = test['userId'].map(user_index).to_numpy()\n",
    "i_te = test['movieId'].map(item_index).to_numpy()\n",
    "y_te = test['rating'].to_numpy().astype(float)\n",
    "y_hat = R_hat[u_te, i_te]\n",
    "\n",
    "rmse_nmf = mean_squared_error(y_te, y_hat, squared=False)\n",
    "print(f\"sklearn NMF (k={k}) — Test RMSE: {rmse_nmf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012df33-254e-44a2-a151-94e392dfd2bc",
   "metadata": {},
   "source": [
    "## Step E — Simple baselines \n",
    "\n",
    "- **Global mean** (predict everyone’s rating as the overall average).\n",
    "- **User mean** (each user’s average).\n",
    "- **Item mean** (each movie’s average).\n",
    "\n",
    "These are strong sanity checks; if NMF can’t beat them, that’s a red flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0da8ffb9-4041-4808-9f54-d6f39404560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean — Test RMSE: 1.1167\n",
      "User mean   — Test RMSE: 1.0370\n",
      "Item mean   — Test RMSE: 0.9796\n"
     ]
    }
   ],
   "source": [
    "global_mean = train['rating'].mean()\n",
    "\n",
    "# user means backed off to global mean if user unseen (shouldn't happen here)\n",
    "user_means = train.groupby('userId')['rating'].mean()\n",
    "item_means = train.groupby('movieId')['rating'].mean()\n",
    "\n",
    "y_hat_global = np.full_like(y_te, global_mean, dtype=float)\n",
    "y_hat_user   = test['userId'].map(user_means).fillna(global_mean).to_numpy()\n",
    "y_hat_item   = test['movieId'].map(item_means).fillna(global_mean).to_numpy()\n",
    "\n",
    "rmse_global = mean_squared_error(y_te, y_hat_global, squared=False)\n",
    "rmse_user   = mean_squared_error(y_te, y_hat_user,   squared=False)\n",
    "rmse_item   = mean_squared_error(y_te, y_hat_item,   squared=False)\n",
    "\n",
    "print(f\"Global mean — Test RMSE: {rmse_global:.4f}\")\n",
    "print(f\"User mean   — Test RMSE: {rmse_user:.4f}\")\n",
    "print(f\"Item mean   — Test RMSE: {rmse_item:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4a2d6-ad42-43a8-b2b1-9e3fb89ba0c7",
   "metadata": {},
   "source": [
    "## Step F — quick sweep over `k`\n",
    "\n",
    "Try a few `n_components` values to see sensitivity of sklearn NMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dda6f7e8-419a-4e61-ba96-52c9c7432d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cynthiamcginnis/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/cynthiamcginnis/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>2.455406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>2.465474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.503531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>2.506790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k      RMSE\n",
       "1  20  2.455406\n",
       "2  40  2.465474\n",
       "0  10  2.503531\n",
       "3  80  2.506790"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = [10, 20, 40, 80]\n",
    "rows = []\n",
    "for k_ in ks:\n",
    "    nmf_ = NMF(n_components=k_, init=\"nndsvda\", max_iter=300, random_state=42)\n",
    "    W_ = nmf_.fit_transform(R_train)\n",
    "    H_ = nmf_.components_\n",
    "    R_ = np.clip(W_ @ H_, 1.0, 5.0)\n",
    "    y_hat_ = R_[u_te, i_te]\n",
    "    rmse_ = mean_squared_error(y_te, y_hat_, squared=False)\n",
    "    rows.append({\"k\": k_, \"RMSE\": rmse_})\n",
    "\n",
    "rmse_table = pd.DataFrame(rows).sort_values(\"RMSE\")\n",
    "rmse_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145af8e-7445-4a64-b515-5c1cd3cdd956",
   "metadata": {},
   "source": [
    "## Step G — Findings: **Limitations of sklearn NMF** (why RMSE may underperform)\n",
    "\n",
    "- **No missing-value mask.** `sklearn`’s NMF minimizes reconstruction error over the *entire* matrix (including zeros we used as “missing”). In CF, zeros aren’t true ratings; this biases the factorization toward predicting many small values and can **hurt RMSE**.\n",
    "- **No per-entry weighting.** We can’t tell NMF to “only care about observed ratings.” (There’s no `sample_weight`/mask in `sklearn` NMF.)\n",
    "- **Rating scale vs. real preferences.** MovieLens ratings are **implicit+explicit** in practice; NMF with squared error on dense (zero-filled) matrices doesn’t match typical CF loss designs (e.g., only on observed entries, or with confidence weights).\n",
    "- **Cold starts & bias terms.** Standard CF benefits from **user/item biases** and sometimes **regularized ALS** over observed entries. Plain NMF here lacks those inductive biases.\n",
    "\n",
    "**Bottom line.** With plain `sklearn` NMF, it’s common to see **RMSE at or above simple baselines** (user/item means). That’s expected due to the constraints above.  scikit learn. (2009). 3.1. Cross-validation: Evaluating Estimator Performance — scikit-learn 0.21.3 Documentation. Scikit-Learn.org. https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d810e-41e5-47ea-8d4f-b2c7ce2dfb1b",
   "metadata": {},
   "source": [
    "## Discussion: Why plain `sklearn` NMF performed poorly vs. simple baselines\n",
    "\n",
    "**Results (this run)**  \n",
    "- `sklearn` NMF (k=20): **RMSE ≈ 2.4554**  \n",
    "- Global mean: **RMSE ≈ 1.1167**  \n",
    "- User mean: **RMSE ≈ 1.0370**  \n",
    "- Item mean: **RMSE ≈ 0.9796**  \n",
    "- NMF sweep: k ∈ {10, 20, 40, 80} → RMSEs ≈ {2.50, 2.46, 2.47, 2.51} (all poor)\n",
    "\n",
    "### Why this happens (core reasons)\n",
    "\n",
    "1. **Wrong objective for matrix completion.**  \n",
    "   `sklearn.decomposition.NMF` minimizes reconstruction error over **every matrix entry**. In sparse ratings, if you fill missings with zeros, NMF also tries to reconstruct those zeros. Because most entries are “missing→0,” the model focuses on explaining zeros instead of **observed** ratings, hurting RMSE on test pairs.\n",
    "\n",
    "2. **No masking of missing entries.**  \n",
    "   True matrix completion minimizes error **only on observed entries**. `sklearn` NMF cannot take a mask of observed indices, so it optimizes the wrong loss when missings are zero-filled.\n",
    "\n",
    "3. **No bias terms (μ, user bias, item bias).**  \n",
    "   Strong baselines use  \n",
    "   $\n",
    "   \\hat r_{ui} = \\mu + b_u + b_i\n",
    "   \\$ \n",
    "   capturing user generosity and item popularity. Plain NMF has **no biases**, so factors must approximate what simple biases explain easily—degrading accuracy.\n",
    "\n",
    "4. **Non-negativity + lack of centering.**  \n",
    "   Recommender models benefit from centering to residuals (above/below zero). NMF requires **non-negative** inputs and factors, preventing proper centering and making it ill-suited for rating residuals.\n",
    "\n",
    "5. **Tuning can’t fix the mismatch.**  \n",
    "   Changing `k` (10→80) doesn’t help because the fundamental **objective mismatch** dominates.\n",
    "\n",
    "---\n",
    "\n",
    "### How to fix / improve\n",
    "\n",
    "**A. Use matrix-completion algorithms with masking + biases**\n",
    "- **SVD/SVD++ with biases** (e.g., Surprise’s `SVD`):  \n",
    "  $\n",
    "  \\hat r_{ui} = \\mu + b_u + b_i + p_u^\\top q_i\n",
    "  \\$ \n",
    "  Optimizes **only on observed entries** with regularization.\n",
    "- **ALS/SGD with an observed-entry mask** (e.g., custom explicit-feedback ALS).\n",
    "\n",
    "**B. Add strong baselines**\n",
    "- **User-/item-mean** (done), plus **kNN (user/item)** on **co-rated** sets (cosine/Pearson). Item-kNN is often very competitive on MovieLens-like data.\n",
    "\n",
    "**C. If you must stay within `sklearn`**\n",
    "- Fit a **bias model** first, then factorize **residuals** with a method that allows negatives (e.g., `TruncatedSVD` on centered residuals), not NMF.\n",
    "- Or use a third-party **masked NMF** implementation.\n",
    "\n",
    "**D. Practical recipe (drop-in path to better RMSE)**\n",
    "\n",
    "1. Estimate global and bias terms on the training set: $\\mu,\\; b_u,\\; b_i$.\n",
    "2. Form residuals for **observed** ratings: $r_{ui} - \\mu - b_u - b_i$.\n",
    "3. Fit a factor model (SVD/ALS) on **observed residuals only** with L2 regularization.\n",
    "4. Predict with the bias-aware factorization:\n",
    "   $\n",
    "   \\hat r_{ui} \\;=\\; \\mu \\;+\\; b_u \\;+\\; b_i \\;+\\; \\mathbf{p}_u^\\top \\mathbf{q}_i.\n",
    "   \\$.\n",
    "5. Evaluate test error with RMSE:\n",
    "   $\n",
    "   \\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{|\\Omega_{\\text{test}}|}\\sum_{(u,i)\\in\\Omega_{\\text{test}}}\\big(r_{ui}-\\hat r_{ui}\\big)^2 }.\n",
    "   \\$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Bottom line\n",
    "Plain `sklearn` NMF is solving the **wrong objective** for recommender data, so it loses to even trivial baselines. Move to a **masked, bias-aware** matrix-completion method (e.g., Surprise SVD / ALS) or use **similarity baselines** (item-kNN) to reach—and typically beat—the item-mean RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbccd38-e00d-4dad-a307-8760c0633282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
